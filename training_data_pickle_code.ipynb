{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_data_pickle_code.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUYUoNRzLSIFFqmuHgVJ6p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GargyaGokhale/Foundation_of_DataMining/blob/master/training_data_pickle_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6twjiy6Sa6d",
        "colab_type": "code",
        "outputId": "5f19830e-90d1-42b9-8e7f-01267bd7d45b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxXqkBD-2ewH",
        "colab_type": "code",
        "outputId": "955e2492-06c7-4280-a6a3-874dc9c9d3cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Thesis_Codes\n",
        "%ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Thesis_Codes\n",
            "dueling_neural_network_class.ipynb  \u001b[0m\u001b[01;34mmodel_pickle_files\u001b[0m/\n",
            "\u001b[01;34mduelling_neural_network\u001b[0m/            training_data_pickle_code.ipynb\n",
            "ev_fleet_dispatch_model.ipynb\n",
            "Collecting mip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/ac/a5b29aa7f6f35a7cad44388bb19313aed830435b9196339e2e47426fd1f6/mip-1.8.2-py3-none-any.whl (47.6MB)\n",
            "\u001b[K     |████████████████████████████████| 47.6MB 104kB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from mip) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->mip) (2.20)\n",
            "Installing collected packages: mip\n",
            "Successfully installed mip-1.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONYOeSH12IQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pprint\n",
        "from sklearn import ensemble\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# from mip.model import *\n",
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import pprint\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNIxlXEXCcHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAvVyjyC4HoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "EV Fleet Model with Dispatch\n",
        "01-03-2020\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"Helper Functions\"\"\"\n",
        "\n",
        "\n",
        "def truncate_normal(mean, sd, min_value, max_value):\n",
        "    \"\"\"Normally distributed random numbers with given mean, standard deviation and max-min\"\"\"\n",
        "    y = np.random.normal(mean, sd, 1)[0]\n",
        "    # y = min_value + (max_value - min_value) * np.random.rand(1)\n",
        "    if y < min_value:\n",
        "        return min_value\n",
        "    elif min_value <= y <= max_value:\n",
        "        return y\n",
        "    else:\n",
        "        return max_value\n",
        "\n",
        "\n",
        "def get_index(element, space):\n",
        "    rows = np.where(element == np.array(space))[0]\n",
        "    return rows[0]\n",
        "\n",
        "\n",
        "\"\"\"EV Class\"\"\"\n",
        "\n",
        "\n",
        "class EV(object):\n",
        "    \"\"\"\n",
        "    Class for individual EVs.\n",
        "    Initialises an EV with normally distributed arrival, departure time\n",
        "        Arrival Time: Between 5- 12 normally distributed at mean  7\n",
        "        Departure Time: Between arrival_time+3- 20 normally distributed at mean 17\n",
        "    Energy required is based on normally distributed distance travelled and the efficiency data of Nissan Leaf\n",
        "        Minimum energy charged is the smallest of normal distribution and the energy_max value\n",
        "            Energy_max = Power * (departure - arrival time)\n",
        "        This ensures all cars come in and leave with energy requirement that can be 100% fulfilled by the environment\n",
        "\n",
        "    Max charging power is 3.0kW\n",
        "\n",
        "    Calculates energy boundary e_max and e_min, corner priority, power(priority)\n",
        "        -Corner Priority is calculated using energy required at time t\n",
        "        and the max energy that can be delivered before departure\n",
        "        Corner priority is updated at the end of every dispatch instruction\n",
        "        -Power(priority) is an ON-OFF dispatch such that\n",
        "            if priority < corner priority  -> Charging ON @max power\n",
        "            else                           -> Charging OFF\n",
        "\n",
        "    Calculates the dispatch power based on the bulk power input.\n",
        "\n",
        "    energy_reset()\n",
        "        Model stores the states of EV for each time step. Use energy_reset() to reset the state tracker\n",
        "        before starting a new run.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.time_vec = np.arange(5, 21)\n",
        "\n",
        "        self.e_max = np.zeros(len(self.time_vec))                     # Energy Envelope\n",
        "        self.e_min = np.zeros(len(self.time_vec))                     # Energy Envelope\n",
        "\n",
        "        self.arrival_time = truncate_normal(8, 2.0, self.time_vec[0], 12)\n",
        "        self.depart_time = truncate_normal(17, 2.0, self.arrival_time+3, self.time_vec[-1])\n",
        "        self.power_max = 3.0                                                    # Maximum charging power in kW\n",
        "        self.current_energy = 0.\n",
        "\n",
        "        energy_max = self.power_max * (self.depart_time - self.arrival_time)    # Maximum Energy possible\n",
        "        energy_req = truncate_normal(80, 100, 40, 172) * 0.174                    # Normally dist. energy\n",
        "        self.energy_req = min(energy_req, energy_max)            # Energy Required for full charge in kWh\n",
        "\n",
        "        self.calculate_energy_boundary()\n",
        "        # self.corner_priority = self.energy_req/((self.depart_time-self.arrival_time)*self.power_max)\n",
        "        self.corner_priority = 0.\n",
        "        self.demand_vec = []  # Power Demand Vector\n",
        "\n",
        "        self.state_tracker = np.zeros(len(self.time_vec))\n",
        "\n",
        "    def calculate_energy_boundary(self):\n",
        "        for time in self.time_vec:\n",
        "            time_index = get_index(time, self.time_vec)\n",
        "            if time < self.arrival_time:\n",
        "                self.e_min[time_index] = 0.\n",
        "                self.e_max[time_index] = 0.\n",
        "            elif self.arrival_time <= time <= self.depart_time:\n",
        "                e_min = max((0.75*self.energy_req - self.power_max*(self.depart_time-time)), 0)\n",
        "                self.e_min[time_index] = e_min\n",
        "\n",
        "                e_max = min(self.energy_req, self.power_max*(time-self.arrival_time))\n",
        "                self.e_max[time_index] = e_max\n",
        "            else:\n",
        "                self.e_min[time_index] = self.energy_req*0.75\n",
        "                self.e_max[time_index] = self.energy_req\n",
        "\n",
        "    def calculate_power(self, priority):\n",
        "        \"\"\"If the corner priority is less than priority then charging switched on, else off\"\"\"\n",
        "        if priority <= self.corner_priority:\n",
        "            i_p = self.power_max\n",
        "        else:\n",
        "            i_p = 0.\n",
        "        return i_p\n",
        "\n",
        "    def dispatch_instruction(self, time, equilibrium_priority):\n",
        "        \"\"\"Dispatches power based on dispatch equilibrium priority and override mechanism\"\"\"\n",
        "        delta_t = 1\n",
        "        time_index = get_index(time, self.time_vec)\n",
        "        power_dispatch = self.calculate_power(equilibrium_priority)\n",
        "        self.current_energy = self.state_tracker[time_index]\n",
        "        next_energy_state = self.current_energy + (power_dispatch * delta_t)\n",
        "\n",
        "        if next_energy_state < self.e_min[time_index+1]:\n",
        "            next_energy_state = self.e_min[time_index+1]\n",
        "            actual_power = (next_energy_state - self.current_energy)/delta_t\n",
        "        elif next_energy_state > self.e_max[time_index+1]:\n",
        "            next_energy_state = self.e_max[time_index + 1]\n",
        "            actual_power = (next_energy_state - self.current_energy) / delta_t\n",
        "        else:\n",
        "            actual_power = power_dispatch\n",
        "\n",
        "        self.state_tracker[time_index + 1] = next_energy_state  # Update state tracker\n",
        "        self.current_energy = next_energy_state\n",
        "        if self.arrival_time <= time < self.depart_time:\n",
        "            self.corner_priority = (self.energy_req - self.current_energy)/((self.depart_time-time)*self.power_max)\n",
        "        elif time == self.depart_time:\n",
        "            self.corner_priority = 0.\n",
        "        elif time < self.arrival_time:\n",
        "            self.corner_priority = 0.\n",
        "        else:\n",
        "            self.corner_priority = 0.\n",
        "\n",
        "        return next_energy_state, actual_power\n",
        "\n",
        "    def energy_reset(self):\n",
        "        self.current_energy = 0.\n",
        "        self.state_tracker = np.zeros(len(self.time_vec))\n",
        "        self.corner_priority = 0.\n",
        "\n",
        "\n",
        "\"\"\"EV Fleet Class\"\"\"\n",
        "\n",
        "\n",
        "class EVFleet(object):\n",
        "    \"\"\"\n",
        "    Initialises the EV Fleet taking input as number of EVs in the fleet\n",
        "    Calculates the fleet energy boundary by aggregating individual EV boundaries\n",
        "    Calculates the demand vector for fleet\n",
        "    Dispatch module calculates equilibrium priority based on input power and passes on to each EV.\n",
        "        -Returns the next energy state of the EV fleet and the actual power used by EVs, to the environment class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, number_evs):\n",
        "        self.n_evs = number_evs\n",
        "        self.ev_list = [EV() for _ in range(self.n_evs)]\n",
        "\n",
        "        self.time_vec = self.ev_list[0].time_vec            # Use time vector from EV Class\n",
        "\n",
        "        self.e_max = np.zeros(len(self.time_vec))\n",
        "        self.e_min = np.zeros(len(self.time_vec))\n",
        "        self.state_tracker = np.zeros(len(self.time_vec))\n",
        "        self.current_energy = 0.\n",
        "\n",
        "        self.priorities = [self.ev_list[n].corner_priority for n in range(self.n_evs)]\n",
        "        self.demand_vec, self.priority_vec = self.calculate_demand()\n",
        "        self.calculate_energy_boundary()\n",
        "\n",
        "    def calculate_energy_boundary(self):\n",
        "\n",
        "        for time_index in range(len(self.time_vec)):\n",
        "            e_min = 0.\n",
        "            e_max = 0.\n",
        "            for ev in self.ev_list:\n",
        "                e_min += ev.e_min[time_index]\n",
        "                e_max += ev.e_max[time_index]\n",
        "\n",
        "            self.e_min[time_index] = e_min\n",
        "            self.e_max[time_index] = e_max\n",
        "\n",
        "    def calculate_demand(self):\n",
        "        max_priority = max(max(self.priorities), 1.0)\n",
        "        priority_step = max_priority / 100\n",
        "        priority_vec = np.arange(0, max_priority+priority_step, priority_step)\n",
        "        demand_vec = np.zeros(len(priority_vec))\n",
        "        priority_index = 0\n",
        "        for priority in priority_vec:\n",
        "            power_priority = 0.\n",
        "            for ev in self.ev_list:\n",
        "                power_priority += ev.calculate_power(priority)\n",
        "            demand_vec[priority_index] = power_priority\n",
        "            priority_index += 1\n",
        "        self.priorities = [self.ev_list[n].corner_priority for n in range(self.n_evs)]\n",
        "        return demand_vec, priority_vec\n",
        "\n",
        "    def calculate_equilibrium_priority(self, power):\n",
        "        priority_index = np.argmin(abs(self.demand_vec - power))\n",
        "        return self.priority_vec[priority_index]\n",
        "\n",
        "    def dispatch_instruction(self, time, power):\n",
        "        time_index = get_index(time, self.time_vec)\n",
        "        equilibrium_priority = self.calculate_equilibrium_priority(power)\n",
        "\n",
        "        next_energy = 0.\n",
        "        actual_power = 0.\n",
        "\n",
        "        for ev in self.ev_list:\n",
        "            e, p = ev.dispatch_instruction(time, equilibrium_priority)\n",
        "            next_energy += e\n",
        "            actual_power += p\n",
        "\n",
        "        real_power = actual_power\n",
        "        self.state_tracker[time_index+1] = next_energy\n",
        "        self.demand_vec, self.priority_vec = self.calculate_demand()\n",
        "\n",
        "        return next_energy, real_power\n",
        "\n",
        "    def no_dispatch_operation(self, time, power):\n",
        "        time_index = get_index(time, self.time_vec)\n",
        "        delta_t = 1\n",
        "        current_energy_state = self.state_tracker[time_index]\n",
        "        next_energy_state = current_energy_state + power * delta_t\n",
        "\n",
        "        if next_energy_state < self.e_min[time_index + 1]:\n",
        "            next_energy_state = self.e_min[time_index + 1]\n",
        "            actual_power = (next_energy_state - current_energy_state) / delta_t\n",
        "        elif next_energy_state > self.e_max[time_index + 1]:\n",
        "            next_energy_state = self.e_max[time_index + 1]\n",
        "            actual_power = (next_energy_state - current_energy_state) / delta_t\n",
        "        else:\n",
        "            actual_power = power\n",
        "\n",
        "        self.state_tracker[time_index+1] = next_energy_state\n",
        "\n",
        "        return next_energy_state, actual_power\n",
        "\n",
        "    def energy_reset(self):\n",
        "        self.state_tracker = np.zeros(len(self.time_vec))\n",
        "        for ev in self.ev_list:\n",
        "            ev.energy_reset()\n",
        "\n",
        "\n",
        "\"\"\"Environment Class\"\"\"\n",
        "\n",
        "\n",
        "class Environment(object):\n",
        "    \"\"\"\n",
        "    Class for environment which initialises a fleet with a number of EVs and the day ahead price to use\n",
        "    Price can be a function or it can be taken from an excel/ csv file during the time of initialising\n",
        "    Class contains reward function, transition function\n",
        "    Transition function takes input action from control algorithm and passes it as dispatch instruction to EV fleet\n",
        "    Returns the next energy state of the fleet and the reward obtained to the control algorithm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_evs):\n",
        "\n",
        "        self.ev_fleet = EVFleet(n_evs)\n",
        "        self.time_vec = self.ev_fleet.time_vec\n",
        "        self.price = self.get_price().copy()\n",
        "        self.state_track = np.zeros(len(self.time_vec))\n",
        "        self.action_track = np.zeros(len(self.time_vec))\n",
        "\n",
        "\n",
        "    def get_price(self):\n",
        "        \"\"\"\n",
        "        price = np.zeros(len(self.time_vec))\n",
        "        time_index = 0\n",
        "        for time in self.time_vec:\n",
        "            price[time_index] = 25+5*np.sin(11*np.pi*time/24 - 0.15) + 3*np.cos(22*np.pi*time/24)\n",
        "            time_index += 1\n",
        "        \"\"\"\n",
        "        price = np.array([26, 30, 19, 18, 30, 26, 30, 17, 19, 27, 32, 5, 5, 27, 28, 24])\n",
        "        return price\n",
        "\n",
        "    def transition(self, time, action):\n",
        "        delta_t = 1\n",
        "        time_index = get_index(time, self.time_vec)\n",
        "        next_state, real_action = self.ev_fleet.dispatch_instruction(time, action)\n",
        "        # next_state, real_action = self.ev_fleet.no_dispatch_operation(time, action)\n",
        "        real_reward = real_action * delta_t * self.price[time_index]\n",
        "        self.state_track[time_index+1] = next_state\n",
        "        self.action_track[time_index] = real_action\n",
        "        return next_state, real_reward\n",
        "\n",
        "    def state_reset(self):\n",
        "        self.state_track = np.zeros(len(self.time_vec))\n",
        "        self.action_track = np.zeros(len(self.time_vec))\n",
        "        self.ev_fleet.energy_reset()\n",
        "\n",
        "\n",
        "def policy(best_action, action_space, threshold):\n",
        "    \"\"\"Epsilon Greedy Policy\"\"\"\n",
        "    if threshold < np.random.rand(1):\n",
        "        return best_action\n",
        "    else:\n",
        "        return action_space[np.random.randint(0, len(action_space))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyjFcm2p7097",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Dueling Neural Network Class\n",
        "20-04-2020\n",
        "Activation Function: tanh\n",
        "Optimiser: Adam\n",
        "\n",
        "Inputs:\n",
        "input_size: Number of input features (int)\n",
        "output_size: Number of output features (int)\n",
        "learning_rate: Initial value of learning rate (float)\n",
        "layer_param: Dictionary for layer sizes for value function, advantage function and fully connected layer\n",
        "            {'v' : [list]\n",
        "            'a' : [list]\n",
        "            'fc': [list]\n",
        "            }\n",
        "            Number of neurons per layer and number of layers (list)\n",
        "            Length of list determines the number of fully connected layers\n",
        "            Each element of the list determines the number of neurons in that layer (int)\n",
        "\n",
        "Architecture:         v----\n",
        "                fc---       }-output\n",
        "                      a====\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"Dueling Neural Network Class\"\"\"\n",
        "\n",
        "\n",
        "class DuelNeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, layer_param, learning_rate):\n",
        "\n",
        "        super(DuelNeuralNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.lr = learning_rate\n",
        "        self.fc_density = layer_param['fc']\n",
        "        self.fc_layers = len(self.fc_density)\n",
        "\n",
        "        self.v_density = layer_param['v']\n",
        "        self.v_layers = len(self.v_density)\n",
        "\n",
        "        self.a_density = layer_param['a']\n",
        "        self.a_layers = len(self.a_density)\n",
        "\n",
        "        self.layer_param = layer_param\n",
        "\n",
        "        \"\"\"Initial fully connected layers\"\"\"\n",
        "\n",
        "        if self.fc_layers == 1:\n",
        "            self.fc1 = nn.Linear(input_size, self.fc_density[0])\n",
        "        elif self.fc_layers == 2:\n",
        "            self.fc1 = nn.Linear(input_size, self.fc_density[0])\n",
        "            self.fc2 = nn.Linear(self.fc_density[0], self.fc_density[1])\n",
        "        elif self.fc_layers == 3:\n",
        "            self.fc1 = nn.Linear(input_size, self.fc_density[0])\n",
        "            self.fc2 = nn.Linear(self.fc_density[0], self.fc_density[1])\n",
        "            self.fc3 = nn.Linear(self.fc_density[1], self.fc_density[2])\n",
        "        else:\n",
        "            \"\"\"Error\"\"\"\n",
        "            raise ValueError(\"Max size of inital fc layers restricted between 1 to 3 fully connected layers\")\n",
        "\n",
        "        \"\"\"Value Function Layers\"\"\"\n",
        "        if self.v_layers == 1:\n",
        "            self.vfc1 = nn.Linear(self.fc_density[-1], self.v_density[0])\n",
        "            self.vfco = nn.Linear(self.v_density[0], 1)\n",
        "        elif self.v_layers == 2:\n",
        "            self.vfc1 = nn.Linear(self.fc_density[-1], self.v_density[0])\n",
        "            self.vfc2 = nn.Linear(self.v_density[0], self.v_density[1])\n",
        "            self.vfco = nn.Linear(self.v_density[1], 1)\n",
        "        elif self.v_layers == 3:\n",
        "            self.vfc1 = nn.Linear(self.fc_density[-1], self.v_density[0])\n",
        "            self.vfc2 = nn.Linear(self.v_density[0], self.v_density[1])\n",
        "            self.vfc3 = nn.Linear(self.v_density[1], self.v_density[2])\n",
        "            self.vfco = nn.Linear(self.v_density[2], 1)\n",
        "        elif self.v_layers == 4:\n",
        "            self.vfc1 = nn.Linear(self.fc_density[-1], self.v_density[0])\n",
        "            self.vfc2 = nn.Linear(self.v_density[0], self.v_density[1])\n",
        "            self.vfc3 = nn.Linear(self.v_density[1], self.v_density[2])\n",
        "            self.vfc4 = nn.Linear(self.v_density[2], self.v_density[3])\n",
        "            self.vfco = nn.Linear(self.v_density[3], 1)\n",
        "        else:\n",
        "            \"\"\"Error\"\"\"\n",
        "            raise ValueError(\"Max size of v-function layer restricted between 1 to 4 fully connected layers\")\n",
        "\n",
        "        \"\"\"Advantage Function Layers\"\"\"\n",
        "        if self.a_layers == 1:\n",
        "            self.afc1 = nn.Linear(self.fc_density[-1], self.a_density[0])\n",
        "            self.afco = nn.Linear(self.a_density[0], output_size)\n",
        "        elif self.a_layers == 2:\n",
        "            self.afc1 = nn.Linear(self.fc_density[-1], self.a_density[0])\n",
        "            self.afc2 = nn.Linear(self.a_density[0], self.a_density[1])\n",
        "            self.afco = nn.Linear(self.a_density[1], output_size)\n",
        "        elif self.a_layers == 3:\n",
        "            self.afc1 = nn.Linear(self.fc_density[-1], self.a_density[0])\n",
        "            self.afc2 = nn.Linear(self.a_density[0], self.a_density[1])\n",
        "            self.afc3 = nn.Linear(self.a_density[1], self.a_density[2])\n",
        "            self.afco = nn.Linear(self.a_density[2], output_size)\n",
        "        elif self.a_layers == 4:\n",
        "            self.afc1 = nn.Linear(self.fc_density[-1], self.a_density[0])\n",
        "            self.afc2 = nn.Linear(self.a_density[0], self.a_density[1])\n",
        "            self.afc3 = nn.Linear(self.a_density[1], self.a_density[2])\n",
        "            self.afc4 = nn.Linear(self.a_density[2], self.a_density[3])\n",
        "            self.afco = nn.Linear(self.a_density[3], output_size)\n",
        "        else:\n",
        "            \"\"\"Error\"\"\"\n",
        "            raise ValueError(\"Max size of a-function layer restricted between 1 to 4 fully connected layers\")\n",
        "\n",
        "\n",
        "        \"\"\"Parameters\"\"\"\n",
        "        self.optimiser = optim.Adam(self.parameters(), lr=self.lr)\n",
        "        # self.optimiser = optim.AdamW(self.parameters(), lr=0.005)\n",
        "        # self.optimiser = torch.optim.RMSprop(self.parameters(), lr=0.005)\n",
        "        self.lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimiser, factor=0.25, patience=50, min_lr=0,\n",
        "                                                                 threshold=1e-4)\n",
        "        self.loss_criteria = nn.MSELoss()\n",
        "        self.eps = 8000\n",
        "        self.best_loss = 1e-4\n",
        "        self.patience = 50\n",
        "        self.early_stop = 0\n",
        "        self.loss_tolerance = 3e-6\n",
        "        self.current_loss = 1\n",
        "\n",
        "        \"\"\"Useful Values\"\"\"\n",
        "        self.vfun = 0.\n",
        "        self.afun = 0.\n",
        "\n",
        "        \"\"\"Graph Parameters\"\"\"\n",
        "        self.loss_list = []\n",
        "        self.ep_list = []\n",
        "\n",
        "        \"\"\"Scale Parameters\"\"\"\n",
        "        self.input_scaler = MinMaxScaler(feature_range=(-1, 1), copy=True)\n",
        "        self.output_scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.fc_layers == 1:\n",
        "            x = torch.tanh(self.fc1(x))\n",
        "        elif self.fc_layers == 2:\n",
        "            x = torch.tanh(self.fc1(x))\n",
        "            x = torch.tanh(self.fc2(x))\n",
        "        elif self.fc_layers == 3:\n",
        "            x = torch.tanh(self.fc1(x))\n",
        "            x = torch.tanh(self.fc2(x))\n",
        "            x = torch.tanh(self.fc3(x))\n",
        "\n",
        "        v_input = x\n",
        "        a_input = x\n",
        "\n",
        "        \"\"\"Value function stream\"\"\"\n",
        "        if self.v_layers == 1:\n",
        "            v_input = torch.tanh(self.vfc1(v_input))\n",
        "        elif self.v_layers == 2:\n",
        "            v_input = torch.tanh(self.vfc1(v_input))\n",
        "            v_input = torch.tanh(self.vfc2(v_input))\n",
        "        elif self.v_layers == 3:\n",
        "            v_input = torch.tanh(self.vfc1(v_input))\n",
        "            v_input = torch.tanh(self.vfc2(v_input))\n",
        "            v_input = torch.tanh(self.vfc3(v_input))\n",
        "        else:\n",
        "            v_input = torch.tanh(self.vfc1(v_input))\n",
        "            v_input = torch.tanh(self.vfc2(v_input))\n",
        "            v_input = torch.tanh(self.vfc3(v_input))\n",
        "            v_input = torch.tanh(self.vfc4(v_input))\n",
        "\n",
        "        self.vfun = self.vfco(v_input)\n",
        "\n",
        "        \"\"\"Advantage Function Stream\"\"\"\n",
        "        if self.a_layers == 1:\n",
        "            a_input = torch.tanh(self.afc1(a_input))\n",
        "        elif self.a_layers == 2:\n",
        "            a_input = torch.tanh(self.afc1(a_input))\n",
        "            a_input = torch.tanh(self.afc2(a_input))\n",
        "        elif self.a_layers == 3:\n",
        "            a_input = torch.tanh(self.afc1(a_input))\n",
        "            a_input = torch.tanh(self.afc2(a_input))\n",
        "            a_input = torch.tanh(self.afc3(a_input))\n",
        "        else:\n",
        "            a_input = torch.tanh(self.afc1(a_input))\n",
        "            a_input = torch.tanh(self.afc2(a_input))\n",
        "            a_input = torch.tanh(self.afc3(a_input))\n",
        "            a_input = torch.tanh(self.afc4(a_input))\n",
        "\n",
        "        self.afun = self.afco(a_input).view(-1, self.output_size)\n",
        "\n",
        "\n",
        "        \"\"\"Aggregating Module Min Type\"\"\"\n",
        "        # Q = V + (A - min A(u) )\n",
        "        minA = (torch.min(self.afun, dim=1)[0]).unsqueeze(dim=1)\n",
        "        minA = minA.expand(self.afun.shape)\n",
        "        output = self.vfun + (self.afun - minA)\n",
        "\n",
        "        '''\n",
        "        \"\"\"Aggregating Module Mean Type\"\"\"\n",
        "        # Q = V + (A - mean A(u) )\n",
        "        meanA = (torch.mean(self.afun, dim=1)).unsqueeze(dim=1)\n",
        "        meanA = meanA.expand(self.afun.shape)\n",
        "        output = self.vfun + (self.afun - meanA)\n",
        "        '''\n",
        "\n",
        "        return output\n",
        "\n",
        "    def training_loop(self, inputs, target, indices):\n",
        "        inputs = inputs.detach().numpy()\n",
        "        inputs = torch.tensor(self.input_scaler.fit_transform(inputs), dtype=torch.float32)\n",
        "        target = target.detach().numpy()\n",
        "        target = torch.tensor(self.output_scaler.fit_transform(target), dtype=torch.float32)\n",
        "        ep = 0\n",
        "        arg_backward = torch.ones(target.shape[1], dtype=torch.float32)\n",
        "        model_lr = self.optimiser.state_dict()['param_groups'][0]['lr']\n",
        "        counter = 0\n",
        "        while ep < self.eps and self.early_stop == 0:\n",
        "            self.optimiser.zero_grad()\n",
        "            feed_forward = self.forward(inputs)\n",
        "            action_indices = (indices.type(torch.long)).view(-1, 1)\n",
        "            output = torch.gather(feed_forward, dim=1, index=action_indices)\n",
        "            loss = self.loss_criteria(output, target)\n",
        "            loss.backward()\n",
        "            self.optimiser.step()\n",
        "            # self.lr_scheduler.step(loss)\n",
        "            # model_lr = self.optimiser.state_dict()['param_groups'][0]['lr']\n",
        "            self.current_loss = loss.detach().numpy()\n",
        "            \"\"\"Early Stopping condition\"\"\"\n",
        "            if self.current_loss < self.best_loss - self.loss_tolerance:\n",
        "                counter = 0\n",
        "                self.best_loss = self.current_loss\n",
        "                self.early_stop = 0\n",
        "            elif (self.best_loss - self.loss_tolerance) <= self.current_loss < self.best_loss:\n",
        "                counter += 1\n",
        "                if counter > self.patience:\n",
        "                    self.early_stop = 1\n",
        "                else:\n",
        "                    self.early_stop = 0\n",
        "            else:\n",
        "                counter = 0\n",
        "                self.early_stop = 0\n",
        "\n",
        "            ep += 1\n",
        "            if ep % 100 == 0:\n",
        "                self.loss_list.append(loss.detach().numpy())\n",
        "                self.ep_list.append(ep)\n",
        "\n",
        "    def scale_input_data(self, input_data):\n",
        "        \"\"\"Takes input as a tensor\n",
        "        Applies MinMax Scaler by first converting into ndarray\n",
        "        Returns a tensor of dtype=float32\n",
        "        \"\"\"\n",
        "        input_data = input_data.detach().numpy()\n",
        "        return torch.tensor(self.input_scaler.transform(input_data), dtype=torch.float32)\n",
        "\n",
        "    def scale_output_data(self, output_data):\n",
        "        \"\"\"Takes input as a tensor\n",
        "        Applies MinMax Scaler by first converting into ndarray\n",
        "        Returns a tensor of dtype=float32\n",
        "        \"\"\"\n",
        "        output_data = output_data.detach().numpy()\n",
        "        return torch.tensor(self.output_scaler.transform(output_data), dtype=torch.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlh-5imMCP2d",
        "colab_type": "code",
        "outputId": "123af81f-44df-45f3-e86f-160a22ce4788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\"\"\" Main Script \"\"\"\n",
        "# Initialise\n",
        "np.random.seed(1)\n",
        "print_obj = pprint.PrettyPrinter(indent=5)\n",
        "n_ev = 100\n",
        "env = Environment(n_ev)\n",
        "\n",
        "t_time = env.time_vec\n",
        "# benchmark_value, benchmark_state = benchmark_solve(min_e, max_e, t_time)\n",
        "U = [30 * i for i in range(11)]  # Action Space\n",
        "\n",
        "# Parameters\n",
        "minimise_sample = len(U)\n",
        "day_duration = len(env.time_vec)\n",
        "\n",
        "number_previous_states = 4\n",
        "\n",
        "size = 2000\n",
        "print(\"Hour training size =\", size)\n",
        "\n",
        "\"\"\"Creating training data\"\"\"\n",
        "training_size_per_hour = size\n",
        "\n",
        "\n",
        "# Sample Template = [time_index, x_t, u_t, r_t, x_t1, previous_states, u_t_index]\n",
        "# Initialise training tensor\n",
        "col_size = 6 + number_previous_states\n",
        "training_set_tensor = torch.zeros([training_size_per_hour * (day_duration - 1), col_size])\n",
        "\n",
        "\n",
        "# Create training samples\n",
        "print(\"Create Training Samples\")\n",
        "plt.figure(\"Training Data Visualisation\")\n",
        "plt.plot(t_time, env.ev_fleet.e_min, label='Minimum Energy', linestyle='--', color='black')\n",
        "plt.plot(t_time, env.ev_fleet.e_max, label='Maximum Energy', linestyle='--', color='black')\n",
        "\n",
        "for sample_index in range(training_size_per_hour):\n",
        "    sample_state_track = [0.]\n",
        "    for hour in range(day_duration - 1):\n",
        "        # print('Hour = ', t_time[hour])\n",
        "        hour_index = hour * training_size_per_hour\n",
        "        if hour == 0:\n",
        "            x_t = 0.\n",
        "            previous_state = [0., 0., 0., 0.]\n",
        "        if sample_index < (training_size_per_hour//3):\n",
        "            u_min = 0\n",
        "            u_max = minimise_sample//2\n",
        "        elif (training_size_per_hour//3) < sample_index < 2*(training_size_per_hour//3):\n",
        "            u_min = minimise_sample//2\n",
        "            u_max = minimise_sample\n",
        "        else:\n",
        "            u_min = 0\n",
        "            u_max = minimise_sample\n",
        "        u_t = U[np.random.randint(u_min, u_max)]\n",
        "        u_t_index = int(get_index(u_t, U))\n",
        "        x_t1, r_t = env.transition(t_time[hour], u_t)\n",
        "\n",
        "        training_set_tensor[hour_index + sample_index, 0] = hour\n",
        "        training_set_tensor[hour_index + sample_index, 1] = x_t\n",
        "        training_set_tensor[hour_index + sample_index, 2] = u_t\n",
        "        training_set_tensor[hour_index + sample_index, 3] = r_t\n",
        "        training_set_tensor[hour_index + sample_index, 4] = x_t1\n",
        "\n",
        "        training_set_tensor[hour_index + sample_index, 5] = previous_state[0]\n",
        "        training_set_tensor[hour_index + sample_index, 6] = previous_state[1]\n",
        "        training_set_tensor[hour_index + sample_index, 7] = previous_state[2]\n",
        "        training_set_tensor[hour_index + sample_index, 8] = previous_state[3]\n",
        "\n",
        "        training_set_tensor[hour_index + sample_index, 9] = u_t_index\n",
        "\n",
        "        previous_state[0] = previous_state[1]\n",
        "        previous_state[1] = previous_state[2]\n",
        "        previous_state[2] = previous_state[3]\n",
        "        previous_state[3] = x_t\n",
        "\n",
        "        x_t = x_t1\n",
        "        sample_state_track.append(x_t)\n",
        "    plt.plot(t_time, sample_state_track, marker='.')\n",
        "\n",
        "    env.state_reset()\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"State of Energy\")\n",
        "\n",
        "\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/Thesis_Codes/model_pickle_files'\n",
        "file_name = \"ev_fleet_model_pickle_\"+str(size)+\"_5_price\"\n",
        "ev_fleet_model_path = os.path.join(data_path, file_name)\n",
        "\n",
        "with open(ev_fleet_model_path, 'wb') as open_file:\n",
        "  pickle.dump(env, file=open_file)\n",
        "\n",
        "file_name = \"training_data_\"+str(size)+\"_5_price\"\n",
        "training_data_path = os.path.join(data_path, file_name)\n",
        "\n",
        "with open(training_data_path, 'wb') as open_file:\n",
        "  pickle.dump(training_set_tensor, file=open_file)\n",
        "\n",
        "\n",
        "print(f\"Environment model pickled at \\n {ev_fleet_model_path}\")\n",
        "print(f\"Training Data pickled at \\n {training_data_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hour training size = 2000\n",
            "Create Training Samples\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}